---
title: "**DATABRICKS INSURANCE**"
output:
   html_document:
    toc: yes
---


```{r, out.width = "600px", echo = FALSE}
knitr::include_graphics("https://i.imgur.com/dDVDTeP.jpeg")
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### **1 Main objective**

In this notebook we're going to explore a car insurance dataset with the objective of explore the usefulness of some machine learning algorithms in the aid of fraud detection.

### **2 Necessary libraries**

Those are the libraries that are necessary to execute all the codes in this notebook.

```{r}
library(jsonlite)
library(ggplot2)
library(lubridate)
```

### **3 Data load**

For this notebook, we're going to merge two datasets obtained from the Databricks' Github repository.

```{r}
policies <- read.csv("https://raw.githubusercontent.com/databricks-industry-solutions/dlt-insurance-claims/refs/heads/main/data/samples/mysql/policies.csv")
claims <- fromJSON("https://raw.githubusercontent.com/databricks-industry-solutions/dlt-insurance-claims/refs/heads/main/data/samples/mongodb/claims.json")
```
### **4 Data exploration**
```{r}
dim(policies)
dim(claims)
```
We have that the dataset "policies" contain 227484 rows and 20 columns, while the dataset "claims" has 50642 rows and 10 columns. Let's see if they can have variables in common.
```{r}
names(policies)
names(claims)
```
Exploring the variable names, we can see that they can have in common the variables "POLICY_NO" from the dataset policies and the variable "policy_no" from the dataset claims. Let's check if they have observations in common in this variables.
```{r}
length(unique(claims$policy_no))
length(unique(policies$POLICY_NO))
```
The number of unique values in the dataset claims is of 45343, so we have 5299 policy numbers that are repeated. Meanwhile in the dataset policies we have 227463 unique values, so there 21 observations are repeated. 

Let's now test if the observations with the same policy numbers in the dataset claims have also the same values for the other variables.
```{r}
head(sort(table(claims$policy_no), decreasing = T))
```
Let's use the policy number 102122085 for the test, since it's the one that appears more times in the dataset.
```{r}
claims[which(claims$policy_no == "102122085"), ]
```
As can be seen the values aren't constant for all the columns through the five rows with repeated policy numbers.

Let's explore the case of the dataset policies now.
```{r}
head(sort(table(policies$POLICY_NO), decreasing = TRUE), 9)
```
15 observations don't have any policy number associated. Let's check the values the other columns take for this observations.
```{r}
policies[which(policies$POLICY_NO == ""), ]
```
Many columns are empty or take the value NA, so we're going to omit these observations before merging both datasets.

Since then only 7 repeated policies remain, this time let's observe visually if these seven policies cotain the same values for each observation.
```{r}
# Let's store the seven policy numbers in a variable
duplicated_policies <- names(head(sort(table(policies$POLICY_NO), decreasing = TRUE), 8)[-1])

na_to_char <- function(input_vector) {
  # This function recieves a row of a data.frame object and if it contains NA values, it returns
  # the same vector with the NA's also as characters.
  # Input by the user is assumed to be correct.
  if (length(input_vector[which(is.na(input_vector))]) > 0) {
    input_vector[which(is.na(input_vector))] <- "NA"
  }
  return(input_vector)
}

checkDiscrepancies <- function(p_numbers) {
  # Input: p_numbers, a character vector with the policy numbers to check.
  # If both rows don't have the same values for each column, a data.frame object
  # with the content of the columns that doesn't match is printed in the console.
  
  for (i in p_numbers) {
    check <- policies[which(policies$POLICY_NO == i), ]
    check[1, ] <- na_to_char(check[1, ])
    check[2, ] <- na_to_char(check[2, ])
    discrepancies <- which(check[1, ] != check[2, ])
    if (length(discrepancies) > 0) {
      cat("Discrepancies for the policy number ", i, " between the two rows found at the variables:\n")
      print("---------------------------------------------------------------------------------------")
      auxiliar_frame <- data.frame(variable_name = colnames(check[discrepancies]), 
                                   row_one_values = unlist(check[1, discrepancies]), 
                                   row_two_values = unlist(check[2, discrepancies]))
      rownames(auxiliar_frame) <- NULL
      print(auxiliar_frame)
      print("---------------------------------------------------------------------------------------")
    } else {
      cat("No discrepancies were found for the policy with the number: ", i)
    }
  }
}

checkDiscrepancies(duplicated_policies)
```

So for simplicity let's remove all the observations with repeated policy numbers before merging both datasets.

Let's store first the policy numbers with repetitions
```{r}
duplicated_pol_claims <- sort(table(claims$policy_no), decreasing = T)
length(duplicated_pol_claims[which(duplicated_pol_claims > 1)])
```
So 4928 observations should be removed from this dataset.
```{r}
duplicated_pol_claims <- duplicated_pol_claims[which(duplicated_pol_claims > 1)]
duplicated_pol_claims <- names(duplicated_pol_claims)
```
We need to store the row indexes of the observations to drop from the dataset.
```{r}
drop_index <- numeric()
for (i in duplicated_pol_claims) {
  drop_index <- c(drop_index, which(claims$policy_no == i))
}
```
Let's add also the duplicated ones in the dataset policies.
```{r}
for (i in duplicated_policies) {
  drop_index <- c(drop_index, which(claims$policy_no == i))
}
```
Which length will have the resulting dataset after the merge without duplicated policies?
```{r}
future_length <- 0
for (i in claims[-drop_index, "policy_no"]) {
  check <- which(policies$POLICY_NO == i)
  if (length(check)) {
    future_length <- future_length + 1
  }
}

print(future_length)
```
So the resulting dataset will have 40412 rows.
Let's proceed now with the merge.`
```{r}
insurance <- merge(claims[-drop_index, ], policies, by.x = c("policy_no"), 
                    by.y = c("POLICY_NO"), all.x = T)
```
Let's check if we have the predicted length for the rows
```{r}
dim(insurance)
```
It looks that there are two extra rows, let's explore if this is caused because of observations
with NA values.
```{r}
which(is.na(insurance$policy_no))
```
Yes, there are two observations with NA as their policy number.
```{r}
print(insurance[which(is.na(insurance$policy_no)), ])
```
Since there are NA values in the majority of the columns for this observations, let's just delete
them form the dataset.
```{r}
insurance <- insurance[-which(is.na(insurance$policy_no)), ]
```
```{r}
dim(insurance)[2]
```
The dataset has 29 different columns. Let's now explore them one by one, so we can know a little more about them.

#### **1 Variable: policy_no**

```{r}
head(insurance$policy_no)
length(unique(insurance$policy_no))
```
As has been seen before, this variable contains the unique policy numbers in string format. This variable looks like useless for modelling or prediction purposes, so we're not going to spend more time on it.

#### **2 Variable: claim_no**
```{r}
head(insurance$claim_no)
length(unique(insurance$claim_no))
```
Here we have again a variable that contains an unique code in string format. So this variable also wouldn't be of any use.

#### **3 Variable: claim_datetime**

```{r}
head(insurance$claim_datetime)
typeof(insurance$claim_datetime)
```
This time we have a variable that contains dates in character format, so let's search 
for a more appropriate data type for them.
First let's convert from character to a date-time object.
```{r}
insurance$claim_datetime <- as_datetime(insurance$claim_datetime)
```
And now let's store the date in one variable and the hour of the claim in another.
Also let's store the day of the week associated to the claim so it could be of any help after.
```{r}
insurance$claim_date <- as_date(insurance$claim_datetime)
insurance$claim_hour <- hour(insurance$claim_datetime)
insurance$claim_wday <- wday(insurance$claim_datetime, week_start = 1)
# Week_start option is used so Monday appear as the first day of week
range(insurance$claim_date)
length(unique(insurance$claim_date))
```
The range of dates of the observations goes from february of 2015 to the last day of 2020.
There are only 1633 different dates for the 40412 observations in the dataset.
Let's observe if all the years are represented equally.
```{r}
# Let's create an auxiliar data.frame for using it with the ggplot function
plot_frame <- data.frame(year = sort(unique(year(insurance$claim_date))))
plot_frame$frequencies <- unname(table(year(insurance$claim_date)))

ggplot(data = plot_frame, aes(x = year, y = frequencies)) +
  geom_bar(stat="identity", color = "blue", fill = "white") +
  ggtitle("Observations for each year")
```

As can be seen here are observations from 2015 to 2020. The years with more registered 
cases are 2016 and 2017 which accumulate the 35% and the 26% of the observations respectivelly.
```{r}
sort(unique(insurance$claim_hour))
length(unique(insurance$claim_hour))
```
There aren't observations for every hour, specifically the hours 24, 2 and 3 are lacking without
any observation.
```{r}
plot_frame <- data.frame(hour = names(table(insurance$claim_hour)))
plot_frame$rel_freqs <- round(100*table(insurance$claim_hour)/length(insurance$claim_hour), 3)
plot_frame$hour <- as.numeric(plot_frame$hour)

ggplot(data = plot_frame, aes(x = hour, y = rel_freqs)) +
  geom_bar(stat="identity", color = "blue", fill = "white") +
  ggtitle("Relative frequencies for each hour")
```

Here observations concentrate on the interval from the 8 a.m. to the 7 p.m. It looks that this coincides with the usual commercial hours. So maybe the majority of people waits to put the claim in these hours and also the majority of incidets occur durying day time.
```{r}
plot_frame <- data.frame(day = 1:7)
plot_frame$rel_freqs <- as.vector(round(100*table(insurance$claim_wday)/length(insurance$claim_wday), 3))


ggplot(data = plot_frame, aes(x = day, y = rel_freqs)) +
  geom_bar(stat="identity", color = "blue", fill = "white") +
  ggtitle("Relative frequencies for each week day")
```

Surprisingly the day with the most claims registered is the Sunday with the 18.660% of them.
The days with lesser claims are Friday and Saturday.

#### **4 Variable: incident**

```{r}
head(insurance$incident)
```
We can see that this variable contains four variables, so let's add each one as a new column to the dataset insurance.
```{r}
insurance$incident_date <- insurance$incident$date
insurance$incident_hour <- insurance$incident$hour
insurance$incident_type <- insurance$incident$type
insurance$incident_severity <- insurance$incident$severity
```
##### **4.1 variable: incident_date**

Since this variable contains dates, let's change its format.
```{r}
insurance$incident_date <- as_date(insurance$incident_date, format = "%d-%m-%Y")
range(insurance$incident_date)
```
We can see that the range of the incidents don't coincide with the one of the claims.
```{r}
plot_frame <- data.frame(year = names(table(year(insurance$incident_date))), 
                         frequencies = as.vector(unname(table(year(insurance$incident_date)))))

ggplot(data = plot_frame, aes(x = year, y = frequencies)) +
  geom_bar(stat="identity", color = "blue", fill = "white") +
  ggtitle("Incidents registered for each year")
```

2016 is again the year with more observations. But this time we have registered incidents as older as 2010, while older claims were of 2015. Let's explore the years of the claims obtained for the incidents older than 2015.

```{r}
subset_incidents <- insurance[which(year(insurance$incident_date) < 2015), c("claim_date", "incident_date")]
dim(subset_incidents)
head(subset_incidents)
```
Let's plot the claim's years for this subset.
```{r}
plot_frame <- data.frame(year = as.numeric(names(table(year(subset_incidents$claim_date)))))
plot_frame$frequencies <- as.vector(table(year(subset_incidents$claim_date)))

ggplot(data = plot_frame, aes(x = year, y = frequencies)) +
  geom_bar(stat="identity", color = "blue", fill = "white") +
  ggtitle("Claim year for incidents registered before 2015")
```

The majority of claims are registered in 2015. Some rare cases are registered in as far as 2020.

Let's check now if all the claims were put after or the same day of the incident, since to have incidents registered after the claim date will not be reasonable and will indicate some kind problem with the data.
```{r}
dates_check_frame <- insurance[, c("claim_date", "incident_date")]
dates_check_frame$differenceDays <- dates_check_frame$claim_date - dates_check_frame$incident_date
range(dates_check_frame$differenceDays)
length(which(dates_check_frame$differenceDays < 0))
```
5456 incident dates are older than the claims.
```{r}
# Histogram for the observations with a less than zero difference.
ggplot(dates_check_frame[which(dates_check_frame$differenceDays < 0), ], aes(x = as.numeric(differenceDays))) +
  geom_histogram(color="darkblue", fill="lightblue", bins = 15) + xlab("days") +
  ggtitle("Days of difference between claim and incident dates")

summary(dates_check_frame$differenceDays[which(dates_check_frame$differenceDays < 0)])
```

The maximum negative difference found between the claim date and the date of the incident is of 365 days, id est, the claim would have been made one year before the incident. Since these information can't be correct and we don't have access to more information about the dates, the more reasonable thing to do seems to put the dates of both columns which return a negative difference as NA.
```{r}
insurance$claim_date[which(dates_check_frame$differenceDays < 0)] <- NA
insurance$incident_date[which(dates_check_frame$differenceDays < 0)] <- NA
```
Let's check again the differences to ensure any negative difference remain.
```{r}
dates_check_frame <- insurance[, c("claim_date", "incident_date")]
dates_check_frame$differenceDays <- dates_check_frame$claim_date - dates_check_frame$incident_date
range(dates_check_frame$differenceDays, na.rm = T)
```
Ok, now there aren't negative differences.
```{r}
# difference between claim date and incident date in days
insurance$diff_ci_days <- insurance$claim_date - insurance$incident_date
range(insurance$diff_ci_days, na.rm = T)
```
```{r}
# Histogram for the differences between the claim date and the incident date.
ggplot(insurance[which(!is.na(insurance$diff_ci_days)), ], aes(x = as.numeric(diff_ci_days))) +
  geom_histogram(color="darkblue", fill="lightblue") + xlab("days") +
  ggtitle("Days of difference between claim and incident dates")

summary(insurance$diff_ci_days)
head(sort(table(insurance$diff_ci_days), decreasing = T), 10)
```

We can see that the 25% of the claims are done in the 7 next days to the incident occurrence.
The 50% are put in the 43 next days to the incident date. The 75% of them are put in the 5 next months to the incident and the maximum of days registered is of 2314 that is approximately more than 6 years which looks like a weird quantity of time waiting to put a claim, but, for example, we could think that in this case the person was severely damaged and couldn't do it before that time. So maybe to know if this information is possible we would need to talk with some expert in insurance policies. Lacking more information, we're gonna take it as a valid value.

As we did with the claim dates, let's store the day of week in which the incident was produced.
Maybe some days are choose more than others to fake incidents.
```{r}
insurance$incident_wday <- wday(insurance$incident_date, week_start = 1)

plot_frame <- data.frame(day = names(table(insurance$incident_wday)))
plot_frame$rel_freq <- as.vector(100*round(table(insurance$incident_wday)/length(insurance$incident_wday), 3))

ggplot(data = plot_frame, aes(x = day, y = rel_freq)) +
  geom_bar(stat="identity", color = "blue", fill = "white") +
  ggtitle("Relative frequencies for each week day.")
```

As in the case of the claims, Friday and Saturday appear as the days with less incidents, and sundays as the day with more.

##### **4.2 variable: incident_hour**

```{r}
length(which(is.na(insurance$incident_hour)))
```
The variable has no NA values.
```{r}
plot_frame <- data.frame(hour = 0:23)
plot_frame$rel_freq <- as.vector(round(100*table(insurance$incident_hour)/length(insurance$incident_hour), 2))

ggplot(data = plot_frame, aes(x = hour, y = rel_freq)) +
  geom_bar(stat="identity", color = "blue", fill = "white") +
  ggtitle("Relative frequencies for incident hour.")

sort(round(100*table(insurance$incident_hour)/length(insurance$incident_hour), 2), decreasing = T)
```

There are incidents registered for any of the 24 hours. None of the hours exceeds to accumulate the 5.44% of the observations. The one that accumulates more observations is 5 p.m. followed by 3 a.m., 12 a.m. and 11 p.m.. It's not surprising that the hour with more incidents is 5 p.m. since it coincides with the end of the work hours for the most people and then when more cars are on the streets, but it seems quite surprising for the late hours to accumulate almost as many incidents. This could be associated to nightlife, so if we see that weekend days accumulate the majority of incidents for these hours, we could give more strength to this hypothesis, let's take a look at it.

```{r}
# Checking the weekdays for the incidents happended at 12:am, 3 a.m. and 23 p.m.
subset_incidents <- numeric()
for (i in c(3, 0, 23)) {
  subset_incidents <- c(subset_incidents, which(insurance$incident_hour == i))
}

# Let's check the subset it's the corrrect
unique(insurance$incident_hour[subset_incidents])

plot_frame <- data.frame(day = names(table(insurance$incident_wday[subset_incidents])))
plot_frame$frequencies <- as.vector(table(insurance$incident_wday[subset_incidents]))

ggplot(data = plot_frame, aes(x = day, y = frequencies)) +
  geom_bar(stat="identity", color = "blue", fill = "white") +
  ggtitle("Day of week frequenciess for incidents occurred at 3 a.m.,\n 12 a.m. or 23 p.m.")
```

The distribution of days looks the same like the one for the whole set of hours.

#### **4.3 incident_type**

```{r}
length(which(is.na(insurance$incident_type)))
```
There are no NA values present in this variable.
```{r}
unique(insurance$incident_type)
```
It contains four different values, which are categorical, so let's change their type from character to factor.
```{r}
insurance$incident_type <- factor(insurance$incident_type)
```
Now let's plot its frequencies.
```{r}
plot_frame <- data.frame(type = names(table(insurance$incident_type)))
plot_frame$frequencies <- as.vector(table(insurance$incident_type))
plot_frame$bar_labels <- as.vector(round(100*table(insurance$incident_type)/length(insurance$incident_type), 2))

ggplot(data = plot_frame, aes(x = type, y = frequencies)) +
  geom_bar(stat="identity", color = "blue", fill = "white") +
  ggtitle("Incident type frequencies.") +
  geom_text(aes(label = scales::percent(bar_labels, scale = 1)), vjust = 1.3)
```

Multi-vehicle collision and single vehicle collision categories occupy more than the 80% of the total observations. Only 9.4% of the observations in the dataset are classified as vehicle theft and 8.3% are classified as parked car, we understand that this means like incidents that took place while the vehicle was stationed.

#### **4.4 incident_severity**

```{r}
length(which(is.na(insurance$incident_severity)))
```
No NA's present in this variable.
```{r}
unique(insurance$incident_severity)
```
Only four different values. This variable it's an ordered categorical, so let's change its type to an ordered factor.
```{r}
insurance$incident_severity <- factor(insurance$incident_severity, 
                                      levels = c("Trivial Damage", "Minor Damage", 
                                                 "Major Damage", "Total Loss"), 
                                      ordered = T)
```
Now let's plot the frequencies for each category to see structure of the variable.
```{r}
frequencies <- round(100*table(insurance$incident_severity)/dim(insurance)[1], 2)
plot_frame <- data.frame(severity = names(frequencies))
plot_frame$frequencies <- as.vector(frequencies)

ggplot(data = plot_frame, aes(x = severity, y = frequencies)) +
  geom_bar(stat="identity", color = "blue", fill = "white") +
  ggtitle("Severity of the incident frequencies.") +
  geom_text(aes(label = scales::percent(frequencies, scale = 1)), vjust = 1.4)

```

Surprisingly, trivial damage is the category with less observations, this can be due to the fact that most policies don't start to cover car's damage until the price is above a certain threshold. The category with most observations is minor damage with the 35.12% of them, and the categories major damage and total loss are almost equal with near the 28% of the observations for each one.

### **5 collision**

```{r}
head(insurance$collision)
```
This variable contains two, so let's add two new columns to the dataset with their information.
```{r}
insurance$collision_type <- insurance$collision$type
insurance$vehicles_involved <- insurance$collision$number_of_vehicles_involved
```

#### **5.1 collision_type**

```{r}
length(which(is.na(insurance$collision_type)))
```
This variable contains 7134 missing values, this is a 17.65% over the total number of rows.
```{r}
unique(insurance$collision_type)
```
It contains only three unique values, apart from the missing ones. Since it's a categorical variable, let's transform its type into an unordered factor.
```{r}
insurance$collision_type <- factor(insurance$collision_type)
```
Now let's plot the frequencies for each category.

```{r}
frequencies <- round(100*table(insurance$collision_type)/dim(insurance)[1], 2)
plot_frame <- data.frame(collision_type = names(frequencies))
plot_frame$frequencies <- as.vector(frequencies)

ggplot(data = plot_frame, aes(x = collision_type, y = frequencies)) +
  geom_bar(stat="identity", color = "blue", fill = "white") +
  ggtitle("Frequencies of the types of collision.") +
  geom_text(aes(label = scales::percent(frequencies, scale = 1)), vjust = 1.4)
```

We can see that the most observed type of collision is rear collision with almost the 29% of the observations followed by side collision with the 28%. Lastly front collision occupy near the 26% of the rows.

To try to know more about the missing observations for this variable, let's analyze the values they take for the variable incident_type.

```{r}
subset_incidents <- insurance$incident_type[which(is.na(insurance$collision_type))]
frequencies <- round(100*table(subset_incidents)/length(subset_incidents), 2)
plot_frame <- data.frame(incident_type = names(frequencies))
plot_frame$frequencies <- as.vector(frequencies)

ggplot(data = plot_frame, aes(x = incident_type, y = frequencies)) +
  geom_bar(stat="identity", color = "blue", fill = "white") +
  ggtitle("Incident type frequencies for the observations with collision type missing.") +
  geom_text(aes(label = scales::percent(frequencies, scale = 1)), vjust = 1.4)
```

Looks pretty similar to the distribution of the rest of the observations.
In this case it could make sense to think that the missing observations are not just missing, but also cases were the collision it's just unkown, like in a case of vandalism were the car was parked in the street or a heavy impact in which the car is so damaged that it's impossible to know the side of the collision or because it was heavy impacted in several ways. So this time, let's try to put all the missing observations into a new category that will be named as 'unknown'.

```{r}
# Let's add again the content into a new column.
insurance$collision_type <- insurance$collision$type
insurance$collision_type <- replace(insurance$collision_type, which(is.na(insurance$collision_type)), "unknown")
insurance$collision_type <- factor(insurance$collision_type)
head(insurance$collision_type)

frequencies <- round(100*table(insurance$collision_type)/dim(insurance)[1], 2)
plot_frame <- data.frame(collision_type = names(frequencies))
plot_frame$frequencies <- as.vector(frequencies)

ggplot(data = plot_frame, aes(x = collision_type, y = frequencies)) +
  geom_bar(stat="identity", color = "blue", fill = "white") +
  ggtitle("Frequencies of the types of collision.") +
  geom_text(aes(label = scales::percent(frequencies, scale = 1)), vjust = 1.4) 
```

#### **5.2 vehicles_involved**

```{r}
length(which(is.na(insurance$vehicles_involved)))
```
There are no NA values in this variable.
```{r}
unique(insurance$vehicles_involved)
```
Surprisingly, this variable only takes two values one, and four. 
```{r}
round(100*table(insurance$vehicles_involved)/length(insurance$vehicles_involved), 2)
```
One appears almost in the 58% o the observations while 4 in the 42%.
