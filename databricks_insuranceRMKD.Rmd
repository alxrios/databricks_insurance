---
title: "**DATABRICKS INSURANCE**"
output:
   html_document:
    toc: yes
---


```{r, out.width = "600px", echo = FALSE}
knitr::include_graphics("https://i.imgur.com/dDVDTeP.jpeg")
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### **1 Main objective**

In this notebook we're going to explore a car insurance dataset with the objective of explore the usefulness of some machine learning algorithms in the aid of fraud detection.

### **2 Necessary libraries**

Those are the libraries that are necessary to execute all the codes in this notebook.

```{r}
library(jsonlite)
library(ggplot2)
library(lubridate)
```

### **3 Data load**

For this notebook, we're going to merge two datasets obtained from the Databricks' Github repository.

```{r}
policies <- read.csv("https://raw.githubusercontent.com/databricks-industry-solutions/dlt-insurance-claims/refs/heads/main/data/samples/mysql/policies.csv")
claims <- fromJSON("https://raw.githubusercontent.com/databricks-industry-solutions/dlt-insurance-claims/refs/heads/main/data/samples/mongodb/claims.json")
```
### **4 Data exploration**
```{r}
dim(policies)
dim(claims)
```
We have that the dataset "policies" contain 227484 rows and 20 columns, while the dataset "claims" has 50642 rows and 10 columns. Let's see if they can have variables in common.
```{r}
names(policies)
names(claims)
```
Exploring the variable names, we can see that they can have in common the variables "POLICY_NO" from the dataset policies and the variable "policy_no" from the dataset claims. Let's check if they have observations in common in this variables.
```{r}
length(unique(claims$policy_no))
length(unique(policies$POLICY_NO))
```
The number of unique values in the dataset claims is of 45343, so we have 5299 policy numbers that are repeated. Meanwhile in the dataset policies we have 227463 unique values, so there 21 observations are repeated. 

Let's now test if the observations with the same policy numbers in the dataset claims have also the same values for the other variables.
```{r}
head(sort(table(claims$policy_no), decreasing = T))
```
Let's use the policy number 102122085 for the test, since it's the one that appears more times in the dataset.
```{r}
claims[which(claims$policy_no == "102122085"), ]
```
As can be seen the values aren't constant for all the columns through the five rows with repeated policy numbers.

Let's explore the case of the dataset policies now.
```{r}
head(sort(table(policies$POLICY_NO), decreasing = TRUE), 9)
```
15 observations don't have any policy number associated. Let's check the values the other columns take for this observations.
```{r}
policies[which(policies$POLICY_NO == ""), ]
```
Many columns are empty or take the value NA, so we're going to omit these observations before merging both datasets.

Since then only 7 repeated policies remain, this time let's observe visually if these seven policies cotain the same values for each observation.
```{r}
# Let's store the seven policy numbers in a variable
duplicated_policies <- names(head(sort(table(policies$POLICY_NO), decreasing = TRUE), 8)[-1])

na_to_char <- function(input_vector) {
  # This function recieves a row of a data.frame object and if it contains NA values, it returns
  # the same vector with the NA's also as characters.
  # Input by the user is assumed to be correct.
  if (length(input_vector[which(is.na(input_vector))]) > 0) {
    input_vector[which(is.na(input_vector))] <- "NA"
  }
  return(input_vector)
}

checkDiscrepancies <- function(p_numbers) {
  # Input: p_numbers, a character vector with the policy numbers to check.
  # If both rows don't have the same values for each column, a data.frame object
  # with the content of the columns that doesn't match is printed in the console.
  
  for (i in p_numbers) {
    check <- policies[which(policies$POLICY_NO == i), ]
    check[1, ] <- na_to_char(check[1, ])
    check[2, ] <- na_to_char(check[2, ])
    discrepancies <- which(check[1, ] != check[2, ])
    if (length(discrepancies) > 0) {
      cat("Discrepancies for the policy number ", i, " between the two rows found at the variables:\n")
      print("---------------------------------------------------------------------------------------")
      auxiliar_frame <- data.frame(variable_name = colnames(check[discrepancies]), 
                                   row_one_values = unlist(check[1, discrepancies]), 
                                   row_two_values = unlist(check[2, discrepancies]))
      rownames(auxiliar_frame) <- NULL
      print(auxiliar_frame)
      print("---------------------------------------------------------------------------------------")
    } else {
      cat("No discrepancies were found for the policy with the number: ", i)
    }
  }
}

checkDiscrepancies(duplicated_policies)
```

So for simplicity let's remove all the observations with repeated policy numbers before merging both datasets.

Let's store first the policy numbers with repetitions
```{r}
duplicated_pol_claims <- sort(table(claims$policy_no), decreasing = T)
length(duplicated_pol_claims[which(duplicated_pol_claims > 1)])
```
So 4928 observations should be removed from this dataset.
```{r}
duplicated_pol_claims <- duplicated_pol_claims[which(duplicated_pol_claims > 1)]
duplicated_pol_claims <- names(duplicated_pol_claims)
```
We need to store the row indexes of the observations to drop from the dataset.
```{r}
drop_index <- numeric()
for (i in duplicated_pol_claims) {
  drop_index <- c(drop_index, which(claims$policy_no == i))
}
```
Let's add also the duplicated ones in the dataset policies.
```{r}
for (i in duplicated_policies) {
  drop_index <- c(drop_index, which(claims$policy_no == i))
}
```
Which length will have the resulting dataset after the merge without duplicated policies?
```{r}
future_length <- 0
for (i in claims[-drop_index, "policy_no"]) {
  check <- which(policies$POLICY_NO == i)
  if (length(check)) {
    future_length <- future_length + 1
  }
}

print(future_length)
```
So the resulting dataset will have 40412 rows.
Let's proceed now with the merge.`
```{r}
insurance <- merge(claims[-drop_index, ], policies, by.x = c("policy_no"), 
                    by.y = c("POLICY_NO"), all.x = T)
```
Let's check if we have the predicted length for the rows
```{r}
dim(insurance)
```
It looks that there are two extra rows, let's explore if this is caused because of observations
with NA values.
```{r}
which(is.na(insurance$policy_no))
```
Yes, there are two observations with NA as their policy number.
```{r}
print(insurance[which(is.na(insurance$policy_no)), ])
```
Since there are NA values in the majority of the columns for this observations, let's just delete
them form the dataset.
```{r}
insurance <- insurance[-which(is.na(insurance$policy_no)), ]
```
```{r}
dim(insurance)[2]
```
The dataset has 29 different columns. Let's now explore them one by one, so we can know a little more about them.

#### **1 Variable: policy_no**

```{r}
head(insurance$policy_no)
length(unique(insurance$policy_no))
```
As has been seen before, this variable contains the unique policy numbers in string format. This variable looks like useless for modelling or prediction purposes, so we're not going to spend more time on it.

#### **2 Variable: claim_no**
```{r}
head(insurance$claim_no)
length(unique(insurance$claim_no))
```
Here we have again a variable that contains an unique code in string format. So this variable also wouldn't be of any use.

#### **3 Variable: claim_datetime**

```{r}
head(insurance$claim_datetime)
typeof(insurance$claim_datetime)
```
This time we have a variable that contains dates in character format, so let's search 
for a more appropriate data type for them.
First let's convert from character to a date-time object.
```{r}
insurance$claim_datetime <- as_datetime(insurance$claim_datetime)
```
And now let's store the date in one variable and the hour of the claim in another.
Also let's store the day of the week associated to the claim so it could be of any help after.
```{r}
insurance$claim_date <- as_date(insurance$claim_datetime)
insurance$claim_hour <- hour(insurance$claim_datetime)
insurance$claim_wday <- wday(insurance$claim_datetime, week_start = 1)
# Week_start option is used so Monday appear as the first day of week
range(insurance$claim_date)
length(unique(insurance$claim_date))
```
The range of dates of the observations goes from february of 2015 to the last day of 2020.
There are only 1633 different dates for the 40412 observations in the dataset.
Let's observe if all the years are represented equally.
```{r}
# Let's create an auxiliar data.frame for using it with the ggplot function
plot_frame <- data.frame(year = sort(unique(year(insurance$claim_date))))
plot_frame$frequencies <- unname(table(year(insurance$claim_date)))

ggplot(data = plot_frame, aes(x = year, y = frequencies)) +
  geom_bar(stat="identity", color = "blue", fill = "white") +
  ggtitle("Observations for each year")
```

As can be seen here are observations from 2015 to 2020. The years with more registered 
cases are 2016 and 2017 which accumulate the 35% and the 26% of the observations respectivelly.
```{r}
sort(unique(insurance$claim_hour))
length(unique(insurance$claim_hour))
```
There aren't observations for every hour, specifically the hours 24, 2 and 3 are lacking without
any observation.
```{r}
plot_frame <- data.frame(hour = names(table(insurance$claim_hour)))
plot_frame$rel_freqs <- round(100*table(insurance$claim_hour)/length(insurance$claim_hour), 3)
plot_frame$hour <- as.numeric(plot_frame$hour)

ggplot(data = plot_frame, aes(x = hour, y = rel_freqs)) +
  geom_bar(stat="identity", color = "blue", fill = "white") +
  ggtitle("Relative frequencies for each hour")
```

Here observations concentrate on the interval from the 8 a.m. to the 7 p.m. It looks that this coincides with the usual commercial hours. So maybe the majority of people waits to put the claim in these hours and also the majority of incidets occur durying day time.
```{r}
plot_frame <- data.frame(day = 1:7)
plot_frame$rel_freqs <- as.vector(round(100*table(insurance$claim_wday)/length(insurance$claim_wday), 3))


ggplot(data = plot_frame, aes(x = day, y = rel_freqs)) +
  geom_bar(stat="identity", color = "blue", fill = "white") +
  ggtitle("Relative frequencies for each week day")
```

Surprisingly the day with the most claims registered is the Sunday with the 18.660% of them.
The days with lesser claims are Friday and Saturday.

#### **4 Variable: incident**

```{r}
head(insurance$incident)
```
We can see that this variable contains four variables, so let's add each one as a new column to the dataset insurance.
```{r}
insurance$incident_date <- insurance$incident$date
insurance$incident_hour <- insurance$incident$hour
insurance$incident_type <- insurance$incident$type
insurance$incident_severity <- insurance$incident$severity
```
##### 4.1 variable: incident_date

Since this variable contains dates, let's change its format.
```{r}
insurance$incident_date <- as_date(insurance$incident_date, format = "%d-%m-%Y")
range(insurance$incident_date)
```
We can see that the range of the incidents don't coincide with the one of the claims.
```{r}
plot_frame <- data.frame(year = names(table(year(insurance$incident_date))), 
                         frequencies = as.vector(unname(table(year(insurance$incident_date)))))

ggplot(data = plot_frame, aes(x = year, y = frequencies)) +
  geom_bar(stat="identity", color = "blue", fill = "white") +
  ggtitle("Incidents registered for each year")
```

2016 is again the year with more observations. But this time we have registered incidents as older as 2010, while older claims were of 2015. Let's explore the years of the claims obtained for the incidents older than 2015.


